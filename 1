from google.colab import drive
drive.mount("/content/drive")
!pip install hanlp
!pip install transformers tokenizers
import os
import re
import json
from collections import Counter, OrderedDict
from tqdm import tqdm

# 数据处理及可视化
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import matplotlib
matplotlib.rc("font", family='SimHei') # 用来显示中文，对于macos系统需要换一个支持的字体

# 自然语言处理
import hanlp
import torch
from transformers import (
    BertTokenizer,
    GPT2LMHeadModel,
    TextGenerationPipeline,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForSeq2SeqLM,
    pipeline
    )
import math

from transformers import BertTokenizer, GPT2LMHeadModel
ckpt_path = "uer/gpt2-chinese-cluecorpussmall" # checkpoint模型路径   #采用的模型为gpt2-chinese-cluecorpussmall
tokenizer = BertTokenizer.from_pretrained(ckpt_path) # 分词器
model = GPT2LMHeadModel.from_pretrained(ckpt_path) # 语言模型

words = ['美丽', '西瓜', '桌子']

# 计算转移概率，使用生成模型的方法
for word in words:
    input_text = "我喜欢吃" + word
    input_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(input_ids)
    
    predicted_logits = outputs.logits[0, -1]  # 取最后一个词的输出
    predicted_probs = torch.softmax(predicted_logits, dim=-1)
    
    word_id = tokenizer.encode(word, add_special_tokens=False)[0]
    word_prob = predicted_probs[word_id].item()
    
    log_word_prob = math.log(word_prob)  # 对数概率
    
    print(f'词语"{word}"的对数概率: {log_word_prob}')

for word in words:
    input_text = "我喜欢吃" + word
    input_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors="pt")
    with torch.no_grad():
        outputs = model(input_ids)
    predicted_logits = outputs.logits[0, -1]  # 取最后一个词的输出
    predicted_probs = torch.softmax(predicted_logits, dim=-1)
    word_id = tokenizer.encode(word, add_special_tokens=False)[0]
    word_prob = predicted_probs[word_id].item()
    log_word_prob = math.log(word_prob)  # 对数概率
    s = -math.log(word_prob)
    print(f'surprisal(我喜欢吃"{word}"): {s}')


## 0. 分词
sent_ex = '这个门被锁了，锁很难被打开。'
tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)
tks = tok(sent_ex)
print('0. 分词结果：')
print(tks)

## 1. 词性标注
pos = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)
print('1. 词性标注：')
print(pos(tks))

context = "人们吃"  #最高频率预测词

# 将上下文文本转换为输入张量
input_ids = tokenizer.encode(context, add_special_tokens=False, return_tensors="pt")

# 使用模型生成下一个词的预测概率分布
with torch.no_grad():
    outputs = model(input_ids)

predicted_logits = outputs.logits[0, -1]  # 获取最后一个词的输出概率分布
predicted_probs = torch.softmax(predicted_logits, dim=-1)  # 转换为概率分布

# 获取概率最高的词的索引
next_word_index = torch.argmax(predicted_probs).item()

# 使用分词器将索引转换为词
next_word = tokenizer.decode([next_word_index])

# 打印下一个可能出现的词和对应的概率
print(f"下一个可能出现的词: {next_word}")
print(f"概率: {predicted_probs[next_word_index].item()}")
